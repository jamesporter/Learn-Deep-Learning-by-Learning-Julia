{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation \n",
    "\n",
    "Backpropagation is a way to training neural networks, where we know the derivatives of each layer. It uses gradient descent as covered in the previous notebook. \n",
    "\n",
    "We'll no longer consider just a single input (a rather unrealistic assumption), but we will consider something fairly simple/restricted: all possible logic (boolean) functions in 4 dimensions.\n",
    "\n",
    "Let's assume we have 4 inputs and a single output. We want to learn for any given input what the output should be.\n",
    "\n",
    "Rather than work with boolean functions, we will create vector/matrix representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bfn_one (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfn_one(a1::Bool, a2::Bool, a3::Bool, a4::Bool) = a1 | a2 & (~a3 | a4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfn_one(true, false, false, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfn_one(false, false, false, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, but let's generate inputs and outputs in number form (0s and 1s) so we can apply our previous techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element Array{NTuple{4,Int64},1}:\n",
       " (0, 0, 0, 0)\n",
       " (0, 0, 0, 1)\n",
       " (0, 0, 1, 0)\n",
       " (0, 0, 1, 1)\n",
       " (0, 1, 0, 0)\n",
       " (0, 1, 0, 1)\n",
       " (0, 1, 1, 0)\n",
       " (0, 1, 1, 1)\n",
       " (1, 0, 0, 0)\n",
       " (1, 0, 0, 1)\n",
       " (1, 0, 1, 0)\n",
       " (1, 0, 1, 1)\n",
       " (1, 1, 0, 0)\n",
       " (1, 1, 0, 1)\n",
       " (1, 1, 1, 0)\n",
       " (1, 1, 1, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = reverse.(Iterators.product(fill(0:1,4)...))[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code might be a bit magical if you aren't familiar with Julia, but let's not get sidetracked... it generates all possible inputs to our function. (Yes, of course I Googled how to do it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NTuple{4,Bool}[(false, false, false, false), (false, false, false, true), (false, false, true, false), (false, false, true, true), (false, true, false, false), (false, true, false, true), (false, true, true, false), (false, true, true, true), (true, false, false, false), (true, false, false, true), (true, false, true, false), (true, false, true, true), (true, true, false, false), (true, true, false, true), (true, true, true, false), (true, true, true, true)], Bool[false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true], [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map single input to Booleans\n",
    "map(Bool, inputs[1])\n",
    "\n",
    "# map all inputs to Booleans\n",
    "inputs_b = map(ip -> map(Bool, ip), inputs)\n",
    "\n",
    "# apply one example\n",
    "bfn_one(map(Bool, inputs[1])...)\n",
    "\n",
    "#apply all\n",
    "outputs_b = map(ib -> bfn_one(ib...), inputs_b)\n",
    "\n",
    "outputs = map(Int, outputs_b)\n",
    "\n",
    "(inputs_b, outputs_b, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we want floating point inputs and outputs (integer makes the zero-ish case unambiguous). Julia allows you to `convert` between types nicely. We also use `collect` to convert a tuple to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array{Float32,1}[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 1.0], [0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 1.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0], [1.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 0.0, 1.0], [1.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 1.0]], Float32[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_f = map(x -> convert(Array{Float32}, collect(x)), inputs)\n",
    "outputs_f = convert(Array{Float32}, outputs)\n",
    "\n",
    "inputs_f, outputs_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. We can get started properly. We are going to pretend we don't know what this function is (and you can try swapping it out for something else and running the below code).\n",
    "\n",
    "To start with let's try to learn this with a single layer network as before. We'll then look at multiple layers, and see backpropagation in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to learn from the entire dataset, rather than just a single input and output as before. To do this we will simply go through all of the input + output pairs in turn, then repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0, 0.0, 0.0, 0.0], 0.0f0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting rows\n",
    "inputs_f[1], outputs_f[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error: 9.363550346863724, weights: [0.453436, -0.063565, 0.263041, 0.0728878]\n",
      "Total error: 3.490099164211415, weights: [0.548826, 0.089625, 0.269098, 0.163558]\n",
      "Total error: 2.41257899721045, weights: [0.588704, 0.173352, 0.231295, 0.192702]\n",
      "Total error: 2.034853159724225, weights: [0.608238, 0.227581, 0.186219, 0.201224]\n",
      "Total error: 1.8283416266677444, weights: [0.619661, 0.266979, 0.144831, 0.202966]\n",
      "Total error: 1.6982437379668867, weights: [0.627366, 0.297423, 0.109676, 0.202591]\n",
      "Total error: 1.6131366577657529, weights: [0.633038, 0.321639, 0.0806658, 0.201654]\n",
      "Total error: 1.5566837736061288, weights: [0.637402, 0.341147, 0.0570024, 0.200657]\n",
      "Total error: 1.5189289996923294, weights: [0.64083, 0.356948, 0.0377918, 0.199751]\n",
      "Total error: 1.4935104605599894, weights: [0.643545, 0.369775, 0.0222271, 0.198968]\n",
      "Total error: 1.4762879777828486, weights: [0.645704, 0.380199, 0.00962687, 0.198307]\n",
      "Total error: 1.4645411634395922, weights: [0.647424, 0.388671, -0.00056976, 0.197752]\n",
      "Total error: 1.456471393061739, weights: [0.648793, 0.39556, -0.00882, 0.197288]\n",
      "Total error: 1.4508838788126073, weights: [0.649884, 0.40116, -0.0154949, 0.196902]\n",
      "Total error: 1.4469816643420412, weights: [0.650752, 0.405714, -0.020895, 0.19658]\n",
      "Total error: 1.44423096970089, weights: [0.651444, 0.409416, -0.0252638, 0.196313]\n",
      "Total error: 1.4422726983729561, weights: [0.651994, 0.412426, -0.028798, 0.196091]\n",
      "Total error: 1.4408640702872226, weights: [0.652432, 0.414873, -0.0316571, 0.195907]\n",
      "Total error: 1.4398400178028907, weights: [0.652779, 0.416863, -0.03397, 0.195754]\n",
      "Total error: 1.4390875893389719, weights: [0.653056, 0.41848, -0.0358411, 0.195627]\n",
      "Total error: 1.438528934644782, weights: [0.653276, 0.419795, -0.0373546, 0.195522]\n",
      "Total error: 1.4381099628844283, weights: [0.65345, 0.420864, -0.038579, 0.195435]\n",
      "Total error: 1.4377927578670753, weights: [0.653588, 0.421733, -0.0395694, 0.195364]\n",
      "Total error: 1.437550486800178, weights: [0.653698, 0.422439, -0.0403705, 0.195304]\n",
      "Total error: 1.4373639678299033, weights: [0.653785, 0.423013, -0.0410185, 0.195255]\n"
     ]
    }
   ],
   "source": [
    "# try different starting points\n",
    "weights = convert(Array{Float32}, [0.2, -0.4, 0.1, -0.2])\n",
    "\n",
    "# try different alphas\n",
    "alpha = 0.05\n",
    "\n",
    "for i = 1:25\n",
    "    err = 0\n",
    "    for j = 1:16\n",
    "        ipt = inputs_f[j]\n",
    "        opt = outputs_f[j]\n",
    "        \n",
    "        prediction = dot(ipt, weights)\n",
    "        \n",
    "        error = (prediction - opt) ^ 2\n",
    "        err += error\n",
    "        \n",
    "        delta = prediction - opt\n",
    "        weights = weights - (alpha * delta * ipt)\n",
    "    end\n",
    "    print(\"Total error: $(err), weights: $(weights)\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a learning process that at least reduces error. The final results don't seem amazing at first, but actually if we round to the nearest value they are right in all but one case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: 0.0, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: 0.19525525950307085, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: -0.04101851377707512, rounded prediction: -0.0, real value: 0.0\n",
      "Predict: 0.15423674572599572, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: 0.42301324600363194, rounded prediction: 0.0, real value: 1.0\n",
      "Predict: 0.6182685055067028, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.38199473222655683, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: 0.5772499917296277, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.6537850102199332, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.8490402697230041, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.6127664964428581, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.808021755945929, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 1.0767982562235652, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 1.272053515726636, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 1.03577974244649, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 1.2310350019495608, rounded prediction: 1.0, real value: 1.0\n"
     ]
    }
   ],
   "source": [
    "for j = 1:16\n",
    "    ipt = inputs_f[j]\n",
    "    opt = outputs_f[j]\n",
    "    prediction = dot(ipt, weights)\n",
    "    print(\"Predict: $(prediction), rounded prediction: $(round(prediction)), real value: $(opt)\\n\")  \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's try something more sophisticated, with multiple layers. We are going to try and dive straight into the code, then figure out what is going on. First we change the activation function to a [relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) function. Previously we just had simplistic linear weightings, which won't work for multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(x) = x > 0 ? x : 0\n",
    "\n",
    "relu(-0.3), relu(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drelu (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drelu(x) = x > 0 ? 1 : 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate random matrices quite simply in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Array{Float32,2}:\n",
       " 0.0685887  0.307153   0.962565  0.504352 \n",
       " 0.0038114  0.149178   0.604692  0.61558  \n",
       " 0.348633   0.0675927  0.969178  0.903942 \n",
       " 0.796645   0.110765   0.349788  0.0240072"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(Array{Float32}, rand(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually have everything we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×4 Transpose{Int64,Array{Int64,1}}:\n",
       " 0  0  0  0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(relu, transpose(inputs_f[1]) * weights_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 2.1956791744951136\n",
      "Error: 1.5184971660158275\n",
      "Error: 1.4018684062932023\n",
      "Error: 1.1563765987532375\n",
      "Error: 0.8659545931347518\n",
      "Error: 0.7286166722972915\n",
      "Error: 0.6477469635292372\n",
      "Error: 0.6036690404020246\n",
      "Error: 0.5867428180375243\n",
      "Error: 0.5641489610331856\n",
      "Error: 0.5619398118862209\n",
      "Error: 0.5549736519180126\n",
      "Error: 0.5491009120140563\n",
      "Error: 0.543861047554249\n",
      "Error: 0.5391048965141519\n",
      "Error: 0.5347594621618874\n",
      "Error: 0.5307696377067468\n",
      "Error: 0.5270871272985168\n",
      "Error: 0.4991148485611913\n",
      "Error: 0.4818549619835947\n",
      "Error: 0.4714584271779233\n",
      "Error: 0.4646841337965765\n",
      "Error: 0.46054234531287686\n",
      "Error: 0.4583334346440867\n",
      "Error: 0.3457354427114695\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "\n",
    "# we are going to add a new layer\n",
    "hiddenSize = 4\n",
    "\n",
    "# we want values between -1 and 1\n",
    "weights_1 = 2 * rand(4, hiddenSize) .- 1\n",
    "weights_2 = 2 * rand(hiddenSize, 1) .- 1\n",
    "\n",
    "for i = 1:250\n",
    "    layer_3_err = 0\n",
    "    for j in 1:16\n",
    "        layer_1 = transpose(inputs_f[j])\n",
    "        layer_2 = map(relu, layer_1 * weights_1)\n",
    "        layer_3 = layer_2 * weights_2        \n",
    "        layer_3_err += (layer_3[1] - outputs_f[j]) .^ 2\n",
    "        layer_3_delta = outputs_f[j] .- layer_3     \n",
    "        layer_2_delta = layer_3_delta * transpose(weights_2) .* map(drelu, layer_2)\n",
    "        weights_2 += alpha * transpose(layer_2) * layer_3_delta\n",
    "        weights_1 += alpha * transpose(layer_1) * layer_2_delta\n",
    "    end    \n",
    "    if i % 10 == 0\n",
    "        println(\"Error: $(layer_3_err)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: 0.0, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: 0.05335071183952486, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: -0.2974743022570351, rounded prediction: -0.0, real value: 0.0\n",
      "Predict: 0.029845521316776615, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: 0.6779849496296484, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.9797686024483634, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.21696932267318514, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: 0.9038419079872269, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.9645602870155497, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.951680820768274, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 1.0521749172538413, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.9944058083323263, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.9972941009265819, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.9928757038445919, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.994140904359529, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 1.004563639621747, rounded prediction: 1.0, real value: 1.0\n"
     ]
    }
   ],
   "source": [
    "for i = 1:16\n",
    "    layer_1 = transpose(inputs_f[i])\n",
    "    layer_2 = map(relu, layer_1 * weights_1)\n",
    "    layer_3 = layer_2 * weights_2\n",
    "    \n",
    "    prediction = layer_3[1]\n",
    "    actual = outputs_f[i]\n",
    "    \n",
    "    print(\"Predict: $(prediction), rounded prediction: $(round(prediction)), real value: $(actual)\\n\")  \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So if we round we are actually right 100% of the time (for this example/learning etc). But how did that work. Let's go through it line by line.\n",
    "\n",
    "`for j in 1:16`\n",
    "\n",
    "Go though each of our 16 examples. With a real dataset we'd probably use batches of samples.\n",
    "\n",
    "`layer_1 = transpose(inputs_f[j])`\n",
    "\n",
    "The first layer is the inputs.\n",
    "\n",
    "`layer_2 = map(relu, layer_1 * weights_1)`\n",
    "\n",
    "The second layer performs `relu(x) = x > 0 ? x : 0` on the first layer multiplied by weightings.\n",
    "\n",
    "`layer_3 = layer_2 * weights_2`        \n",
    "\n",
    "The third layer (the output) is just a linear weighting of the previous output.\n",
    "\n",
    "`layer_3_err += (layer_3[1] - outputs_f[j]) .^ 2`\n",
    "\n",
    "The error is the mean squared mismatch.\n",
    "\n",
    "`layer_3_delta = outputs_f[j] .- layer_3`\n",
    "\n",
    "We weight our overall adjustment by the error in the output. \n",
    "\n",
    "`layer_2_delta = layer_3_delta * transpose(weights_2) .* map(drelu, layer_2)`\n",
    "\n",
    "Now we work out the adjustment to the first set of weightings by taking the derivated of the second layer operation mutliplied by the weightings.\n",
    "\n",
    "`weights_2 += alpha * transpose(layer_2) * layer_3_delta`\n",
    "\n",
    "We adjust the second layer. Note the alpha term which controls speed of adjustment.\n",
    "\n",
    "`weights_1 += alpha * transpose(layer_1) * layer_2_delta`\n",
    "\n",
    "And first layer.\n",
    "\n",
    "But all of this happens for all of the data. We aren't just updating with a single input/output. We try and move in right overall direction.\n",
    "\n",
    "The key point is we work backwards adjusting the weights towards better values that reduce error in the output (or adjust in the right direction for the next layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We'll look at how we do this kind of thing for real in the next section. While looking at the low level details are useful both for learning Julia and understanding the core principles it would be extremely tedious and error prone day to day.\n",
    "\n",
    "We'll use [Flux](https://github.com/FluxML/Flux.jl), a Julia framework for machine learning. It is pure Julia yet can benefit from GPU accleration. It allows us to build and train models in a remarkably concise and clear way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
