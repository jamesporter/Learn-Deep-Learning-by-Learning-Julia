{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation \n",
    "\n",
    "Backpropagation is a way to training neural networks, where we know the derivatives of each layer. It uses gradient descent as covered in the previous notebook. \n",
    "\n",
    "We'll no longer consider just a single input (a rather unrealistic assumption), but we will consider something fairly simple/restricted: all possible logic (boolean) functions in 4 dimensions.\n",
    "\n",
    "Let's assume we have 4 inputs and a single output. We want to learn for any given input what the output should be.\n",
    "\n",
    "Rather than work with boolean functions, we will create vector/matrix representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bfn_one (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfn_one(a1::Bool, a2::Bool, a3::Bool, a4::Bool) = a1 | a2 & (~a3 | a4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfn_one(true, false, false, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfn_one(false, false, false, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, but let's generate inputs and outputs in number form (0s and 1s) so we can apply our previous techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element Array{NTuple{4,Int64},1}:\n",
       " (0, 0, 0, 0)\n",
       " (0, 0, 0, 1)\n",
       " (0, 0, 1, 0)\n",
       " (0, 0, 1, 1)\n",
       " (0, 1, 0, 0)\n",
       " (0, 1, 0, 1)\n",
       " (0, 1, 1, 0)\n",
       " (0, 1, 1, 1)\n",
       " (1, 0, 0, 0)\n",
       " (1, 0, 0, 1)\n",
       " (1, 0, 1, 0)\n",
       " (1, 0, 1, 1)\n",
       " (1, 1, 0, 0)\n",
       " (1, 1, 0, 1)\n",
       " (1, 1, 1, 0)\n",
       " (1, 1, 1, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = reverse.(Iterators.product(fill(0:1,4)...))[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code might be a bit magical if you aren't familiar with Julia, but let's not get sidetracked... it generates all possible inputs to our function. (Yes, of course I Googled how to do it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(NTuple{4,Bool}[(false, false, false, false), (false, false, false, true), (false, false, true, false), (false, false, true, true), (false, true, false, false), (false, true, false, true), (false, true, true, false), (false, true, true, true), (true, false, false, false), (true, false, false, true), (true, false, true, false), (true, false, true, true), (true, true, false, false), (true, true, false, true), (true, true, true, false), (true, true, true, true)], Bool[false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true], [0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map single input to Booleans\n",
    "map(Bool, inputs[1])\n",
    "\n",
    "# map all inputs to Booleans\n",
    "inputs_b = map(ip -> map(Bool, ip), inputs)\n",
    "\n",
    "# apply one example\n",
    "bfn_one(map(Bool, inputs[1])...)\n",
    "\n",
    "#apply all\n",
    "outputs_b = map(ib -> bfn_one(ib...), inputs_b)\n",
    "\n",
    "outputs = map(Int, outputs_b)\n",
    "\n",
    "(inputs_b, outputs_b, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we want floating point inputs and outputs (integer makes the zero-ish case unambiguous). Julia allows you to `convert` between types nicely. We also use `collect` to convert a tuple to an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array{Float32,1}[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 1.0], [0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 1.0], [1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0], [1.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 0.0, 1.0], [1.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 1.0]], Float32[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_f = map(x -> convert(Array{Float32}, collect(x)), inputs)\n",
    "outputs_f = convert(Array{Float32}, outputs)\n",
    "\n",
    "inputs_f, outputs_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. We can get started properly. We are going to pretend we don't know what this function is (and you can try swapping it out for something else and running the below code).\n",
    "\n",
    "To start with let's try to learn this with a single layer network as before. We'll then look at multiple layers, and see backpropagation in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to learn from the entire dataset, rather than just a single input and output as before. To do this we will simply go through all of the input + output pairs in turn, then repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.0, 0.0, 0.0, 0.0], 0.0f0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting rows\n",
    "inputs_f[1], outputs_f[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error: 9.363550346863724, weights: [0.453436, -0.063565, 0.263041, 0.0728878]\n",
      "Total error: 3.490099164211415, weights: [0.548826, 0.089625, 0.269098, 0.163558]\n",
      "Total error: 2.41257899721045, weights: [0.588704, 0.173352, 0.231295, 0.192702]\n",
      "Total error: 2.034853159724225, weights: [0.608238, 0.227581, 0.186219, 0.201224]\n",
      "Total error: 1.8283416266677444, weights: [0.619661, 0.266979, 0.144831, 0.202966]\n",
      "Total error: 1.6982437379668867, weights: [0.627366, 0.297423, 0.109676, 0.202591]\n",
      "Total error: 1.6131366577657529, weights: [0.633038, 0.321639, 0.0806658, 0.201654]\n",
      "Total error: 1.5566837736061288, weights: [0.637402, 0.341147, 0.0570024, 0.200657]\n",
      "Total error: 1.5189289996923294, weights: [0.64083, 0.356948, 0.0377918, 0.199751]\n",
      "Total error: 1.4935104605599894, weights: [0.643545, 0.369775, 0.0222271, 0.198968]\n",
      "Total error: 1.4762879777828486, weights: [0.645704, 0.380199, 0.00962687, 0.198307]\n",
      "Total error: 1.4645411634395922, weights: [0.647424, 0.388671, -0.00056976, 0.197752]\n",
      "Total error: 1.456471393061739, weights: [0.648793, 0.39556, -0.00882, 0.197288]\n",
      "Total error: 1.4508838788126073, weights: [0.649884, 0.40116, -0.0154949, 0.196902]\n",
      "Total error: 1.4469816643420412, weights: [0.650752, 0.405714, -0.020895, 0.19658]\n",
      "Total error: 1.44423096970089, weights: [0.651444, 0.409416, -0.0252638, 0.196313]\n",
      "Total error: 1.4422726983729561, weights: [0.651994, 0.412426, -0.028798, 0.196091]\n",
      "Total error: 1.4408640702872226, weights: [0.652432, 0.414873, -0.0316571, 0.195907]\n",
      "Total error: 1.4398400178028907, weights: [0.652779, 0.416863, -0.03397, 0.195754]\n",
      "Total error: 1.4390875893389719, weights: [0.653056, 0.41848, -0.0358411, 0.195627]\n",
      "Total error: 1.438528934644782, weights: [0.653276, 0.419795, -0.0373546, 0.195522]\n",
      "Total error: 1.4381099628844283, weights: [0.65345, 0.420864, -0.038579, 0.195435]\n",
      "Total error: 1.4377927578670753, weights: [0.653588, 0.421733, -0.0395694, 0.195364]\n",
      "Total error: 1.437550486800178, weights: [0.653698, 0.422439, -0.0403705, 0.195304]\n",
      "Total error: 1.4373639678299033, weights: [0.653785, 0.423013, -0.0410185, 0.195255]\n"
     ]
    }
   ],
   "source": [
    "# try different starting points\n",
    "weights = convert(Array{Float32}, [0.2, -0.4, 0.1, -0.2])\n",
    "\n",
    "# try different alphas\n",
    "alpha = 0.05\n",
    "\n",
    "for i = 1:25\n",
    "    err = 0\n",
    "    for j = 1:16\n",
    "        ipt = inputs_f[j]\n",
    "        opt = outputs_f[j]\n",
    "        \n",
    "        prediction = dot(ipt, weights)\n",
    "        \n",
    "        error = (prediction - opt) ^ 2\n",
    "        err += error\n",
    "        \n",
    "        delta = prediction - opt\n",
    "        weights = weights - (alpha * delta * ipt)\n",
    "    end\n",
    "    print(\"Total error: $(err), weights: $(weights)\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a learning process that at least reduces error. The final results don't seem amazing at first, but actually if we round to the nearest value they are right in all but one case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: 0.0, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: 0.19525525950307085, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: -0.04101851377707512, rounded prediction: -0.0, real value: 0.0\n",
      "Predict: 0.15423674572599572, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: 0.42301324600363194, rounded prediction: 0.0, real value: 1.0\n",
      "Predict: 0.6182685055067028, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.38199473222655683, rounded prediction: 0.0, real value: 0.0\n",
      "Predict: 0.5772499917296277, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.6537850102199332, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.8490402697230041, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.6127664964428581, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 0.808021755945929, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 1.0767982562235652, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 1.272053515726636, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 1.03577974244649, rounded prediction: 1.0, real value: 1.0\n",
      "Predict: 1.2310350019495608, rounded prediction: 1.0, real value: 1.0\n"
     ]
    }
   ],
   "source": [
    "for j = 1:16\n",
    "    ipt = inputs_f[j]\n",
    "    opt = outputs_f[j]\n",
    "    prediction = dot(ipt, weights)\n",
    "    print(\"Predict: $(prediction), rounded prediction: $(round(prediction)), real value: $(opt)\\n\")  \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's try something more sophisticated, with multiple layers. We are going to try and dive straight into the code, then figure out what is going on. First we change the activation function to a [relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) function. Previously we just had simplistic linear weightings, which won't work for multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(x) = x > 0 ? x : 0\n",
    "\n",
    "relu(-0.3), relu(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drelu (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drelu(x) = x > 0 ? 1 : 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually have everything we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
